<?xml version="1.0" encoding="UTF-8"?>
<configuration>
<property>
<name>hive.metastore.db.ssl.enabled</name>
<value>false</value>
</property>
<property>
<name>spark.thriftserver.retry.wait.time</name>
<value>10</value>
</property>
<property>
<name>hive.security.authenticator.manager</name>
<value>org.apache.hadoop.hive.ql.security.SessionStateUserGroupAuthenticator</value>
</property>
<property>
<name>hive.metastore.sasl.enabled</name>
<value>true</value>
</property>
<property>
<name>hive.security.authorization.manager</name>
<value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory</value>
</property>
<property>
<name>hive.server2.authentication</name>
<value>KERBEROS</value>
</property>
<property>
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
</property>
<property>
<name>hive.metastore.thrift.sasl.qop</name>
<value>auth-conf</value>
</property>
<property>
<name>hive.metastore.uris</name>
<value>thrift://187.5.89.66:21088,thrift://187.5.89.47:21088</value>
</property>
<property>
<name>spark.proxyserver.hash.enabled</name>
<value>true</value>
</property>
<property>
<name>hive.server2.authentication.kerberos.principal</name>
<value>spark2x/hadoop.hadoop.com@HADOOP.COM</value>
</property>
<property>
<name>spark.thriftserver.retry.times</name>
<value>5</value>
</property>
<property>
<name>hive.metastore.token.signature</name>
<value>HiveServer2ImpersonationToken</value>
</property>
<property>
<name>hive.metastore.rdb.password.decode.enable</name>
<value>true</value>
</property>
<property>
<name>spark.thriftserver.ha.enabled</name>
<value>false</value>
</property>
<property>
<name>hive.server2.authentication.kerberos.keytab</name>
<value>/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/keytab/spark2x/SparkResource/spark2x.keytab</value>
</property>
<property>
<name>hive.metastore.kerberos.principal</name>
<value>hive/hadoop.hadoop.com@HADOOP.COM</value>
</property>
<property>
<name>spark.deploy.zookeeper.url</name>
<value>187.5.89.47:24002,187.5.89.12:24002,187.5.89.66:24002</value>
</property>
<property>
<name>hive.security.authorization.enabled</name>
<value>true</value>
</property>
<property>
<name>hive.server2.thrift.sasl.qop</name>
<value>auth-conf</value>
</property>
<property>
<name>hive.security.authorization.sqlstd.confwhitelist.append</name>
<value>hbase\.compact\.index\.combine|hbase\.composite\.key\.factory|hive\.map\.aggr\.hash\.percentmemory|hive\.map\.aggr\.hash\.force\.flush\.memory\.threshold|hive\.map\.aggr\.hash\.min\.reduction|hive\.multigroupby\.singlereducer|hive\.map\.groupby\.sorted|hive\.map\.groupby\.sorted\.testmode|hive\.exec\.mode\.splits\.local\.auto|mapreduce\.framework\.name|hive\.aux\.jars\.path|hive\.smalltable\.filesize|hive\.mapred\.map\.tasks\.speculative\.execution|hive\.mapred\.reduce\.tasks\.speculative\.execution|mapreduce\.job\.ubertask\.enable|mapreduce\.job\.ubertask\.maxmaps|mapreduce\.job\.ubertask\.maxreduces|hive\.textinput\.record\.delimiter|dfs\.block\.size|mapreduce\.input\.fileinputformat\.split\.maxsize|mapreduce\.task\.io\.sort\.mb|hive\.input\.format|orc\.output\.codec|hive\.exec\.max\.dynamic\.partitions|inner\.client\.marker</value>
</property>
<property>
<name>hive.authorization.msck.enabled</name>
<value>true</value>
</property>
<property>
<name>hive.security.authorization.createtable.owner.grants</name>
<value>ALL</value>
</property>
<property>
<name>hive.server2.thrift.port</name>
<value>22550</value>
</property>
<property>
<name>hive.server2.enable.doAs</name>
<value>true</value>
</property>
<property>
<name>hive.exec.scratchdir</name>
<value>/tmp/sparkhive-scratch</value>
</property>
<property>
<name>spark.thriftserver.zookeeper.dir</name>
<value>/thriftserver2x</value>
</property>
<property>
<name>hive.server2.thrift.bind.host</name>
<value>187.5.89.12</value>
</property>
</configuration>
